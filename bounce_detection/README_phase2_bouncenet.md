# BounceNet æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨ (Phase 2)

**BounceNet - Deep Learning Event Classifier for Badminton**

å¯¹ Phase 1 è§„åˆ™æ£€æµ‹ç”Ÿæˆçš„å€™é€‰äº‹ä»¶è¿›è¡Œåˆ†ç±»ï¼Œè¿‡æ»¤è¯¯æ£€å¹¶ä¿®æ­£äº‹ä»¶ç±»å‹ã€‚

---

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [ç½‘ç»œæ¶æ„](#ç½‘ç»œæ¶æ„)
- [ç‰¹å¾æå–](#ç‰¹å¾æå–)
- [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
- [æ¨ç†ä½¿ç”¨](#æ¨ç†ä½¿ç”¨)
- [API å‚è€ƒ](#api-å‚è€ƒ)
- [å®éªŒç»“æœ](#å®éªŒç»“æœ)

---

## æ¦‚è¿°

### åŠ¨æœº

Phase 1 è§„åˆ™æ£€æµ‹è¿½æ±‚**é«˜å¬å›ç‡**ï¼Œä¼šäº§ç”Ÿä¸€äº›è¯¯æ£€ã€‚BounceNet çš„ç›®æ ‡æ˜¯ï¼š

1. **è¿‡æ»¤è¯¯æ£€**ï¼šè¯†åˆ«å¹¶ç§»é™¤éäº‹ä»¶å€™é€‰
2. **ä¿®æ­£åˆ†ç±»**ï¼šæ›´å‡†ç¡®åœ°åŒºåˆ† landing/hit ç±»å‹
3. **æä¾›ç½®ä¿¡åº¦**ï¼šä¸ºæ¯ä¸ªé¢„æµ‹ç»™å‡ºå¯é æ€§è¯„ä¼°

### ä¸‰ç§å·¥ä½œæ¨¡å¼

| æ¨¡å¼ | è¾“å…¥ç‰¹å¾ | å‚æ•°é‡ | é€‚ç”¨åœºæ™¯ |
|------|---------|--------|---------|
| `trajectory_only` | è½¨è¿¹åºåˆ— | ~196K | å¿«é€Ÿæ¨ç†ï¼Œæ— éœ€è§†é¢‘ |
| `visual_only` | è§†é¢‘ patch | ~350K | è§†è§‰çº¿ç´¢ä¸ºä¸» |
| `fusion` | è½¨è¿¹ + è§†é¢‘ | ~460K | æœ€é«˜ç²¾åº¦ |

### åˆ†ç±»ç›®æ ‡

| ç±»åˆ« | Label | æè¿° |
|------|-------|------|
| `none` | 0 | éäº‹ä»¶ï¼ˆè¯¯æ£€ï¼‰ |
| `landing` | 1 | è½åœ°ç‚¹ |
| `hit` | 2 | å‡»çƒç‚¹ |

---

## ç½‘ç»œæ¶æ„

### æ•´ä½“æ¶æ„

```
è¾“å…¥ç‰¹å¾
    â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                                         â”‚
    â–¼                                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Trajectory Encoder â”‚           â”‚   Visual Encoder    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚           â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚
â”‚  â€¢ 1D Conv (é¢„å¤„ç†)  â”‚           â”‚  â€¢ Patch Encoder    â”‚
â”‚  â€¢ Bi-LSTM (æ—¶åº)    â”‚           â”‚  â€¢ Temporal Pooling â”‚
â”‚  â€¢ FC (æŠ•å½±)         â”‚           â”‚  â€¢ Attention        â”‚
â”‚        â†“             â”‚           â”‚         â†“           â”‚
â”‚   (N, 64)           â”‚           â”‚    (N, 128)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚                                         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ Concatenate
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Classifier  â”‚
            â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
            â”‚  â€¢ FC + BN    â”‚
            â”‚  â€¢ Dropout    â”‚
            â”‚  â€¢ FC â†’ 3     â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   Output      â”‚
            â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚
            â”‚  â€¢ logits (3) â”‚
            â”‚  â€¢ confidence â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### TrajectoryEncoder

```python
class TrajectoryEncoder(nn.Module):
    """
    è¾“å…¥: (N, seq_len, 5)  # [x, y, v_x, v_y, visibility]
    è¾“å‡º: (N, feature_dim)
    
    ç»“æ„:
    - 1D Conv: é¢„å¤„ç† + å±€éƒ¨ç‰¹å¾
    - Bi-LSTM: æ—¶åºå»ºæ¨¡
    - FC: ç‰¹å¾æŠ•å½±
    """
```

**è¾“å…¥ç‰¹å¾** (5ç»´):
| é€šé“ | ç‰¹å¾ | å½’ä¸€åŒ– |
|------|------|--------|
| 0 | x åæ ‡ | / img_width |
| 1 | y åæ ‡ | / img_height |
| 2 | x æ–¹å‘é€Ÿåº¦ | / å¯¹è§’çº¿é•¿åº¦ |
| 3 | y æ–¹å‘é€Ÿåº¦ | / å¯¹è§’çº¿é•¿åº¦ |
| 4 | å¯è§æ€§ | 0 æˆ– 1 |

### VisualEncoder

```python
class TemporalPatchEncoder(nn.Module):
    """
    è¾“å…¥: (N, seq_len, 3, 64, 64)  # æ—¶åº RGB patches
    è¾“å‡º: (N, feature_dim)
    
    ç»“æ„:
    - PatchEncoder: æ¯å¸§ç‹¬ç«‹ç¼–ç  (å…±äº«æƒé‡)
    - 1D Conv: æ—¶åºå·ç§¯
    - Attention Pooling: åŠ æƒèšåˆ
    """
```

---

## ç‰¹å¾æå–

### VisualFeatureExtractor

ä»è§†é¢‘å¸§ä¸­æå–å€™é€‰äº‹ä»¶å‘¨å›´çš„è§†è§‰ç‰¹å¾ã€‚

```python
from bounce_detection import VisualFeatureExtractor

extractor = VisualFeatureExtractor(
    patch_size=64,         # patch å¤§å°
    temporal_window=5,     # æ—¶åºçª—å£ï¼ˆå•ä¾§ï¼‰
    use_motion_history=True,
    img_size=(512, 288)
)

# æå–å•ä¸ªå€™é€‰çš„ç‰¹å¾
features = extractor.extract_features(
    frames,           # è§†é¢‘å¸§åˆ—è¡¨
    x_coords,         # x åæ ‡åºåˆ—
    y_coords,         # y åæ ‡åºåˆ—
    center_frame_idx, # å€™é€‰å¸§ç´¢å¼•
    visibility        # å¯è§æ€§åºåˆ—
)

# è¿”å›:
# - center_patch: (64, 64, 3) ä¸­å¿ƒå¸§ patch
# - temporal_patches: (11, 64, 64, 3) æ—¶åº patches
# - mhi_patch: (64, 64) è¿åŠ¨å†å²å›¾
# - center_coords: (x_norm, y_norm) å½’ä¸€åŒ–åæ ‡
```

### ç‰¹å¾ç±»å‹

| ç‰¹å¾ | å½¢çŠ¶ | æè¿° |
|------|------|------|
| `center_patch` | (H, W, 3) | å€™é€‰å¸§å¤„ä»¥çƒä¸ºä¸­å¿ƒçš„ RGB patch |
| `temporal_patches` | (T, H, W, 3) | è¿ç»­ T å¸§çš„ patch åºåˆ— |
| `mhi_patch` | (H, W) | è¿åŠ¨å†å²å›¾ï¼Œç¼–ç è¿‘æœŸè¿åŠ¨ |

### è¿åŠ¨å†å²å›¾ (MHI)

```
MHI ç¼–ç åŸç†:
- åƒç´ å€¼è¡¨ç¤ºè¯¥ä½ç½®æœ€è¿‘å‘ç”Ÿè¿åŠ¨çš„æ—¶é—´
- è¶Šäº® = è¿åŠ¨è¶Šè¿‘æœŸ
- æ•æ‰çƒçš„è¿åŠ¨è½¨è¿¹å’Œé€Ÿåº¦ä¿¡æ¯

        æ—¶é—´ â†’
å¸§ t-4  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  (æš—)
å¸§ t-3  â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  
å¸§ t-2  â–’â–’â–’â–’â–’â–’â–’â–’  (ä¸­ç­‰)
å¸§ t-1  â–“â–“â–“â–“â–“â–“â–“â–“  
å¸§ t    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  (äº®)
```

---

## è®­ç»ƒæµç¨‹

### æ•°æ®å‡†å¤‡

1. **ä½¿ç”¨æ ‡æ³¨å·¥å…·ç”Ÿæˆè®­ç»ƒæ•°æ®**:
```bash
# å¯åŠ¨æ ‡æ³¨å·¥å…·
python labeling_launcher.py --csv data/train/match1/csv/1_01_01_ball.csv

# æ ‡æ³¨åä¼šç”Ÿæˆ JSON æ–‡ä»¶:
# labels/match1/1_01_01_ball_labels.json
```

2. **JSON æ ‡æ³¨æ ¼å¼**:
```json
{
  "csv_path": "data/train/match1/csv/1_01_01_ball.csv",
  "video_path": "data/train/match1/video/1_01_01.mp4",
  "events": [
    {
      "frame": 45,
      "event_type": "hit",
      "confirmed": true,
      "x": 256.5,
      "y": 180.2
    },
    {
      "frame": 120,
      "event_type": "landing",
      "confirmed": true,
      "x": 320.1,
      "y": 250.8
    }
  ]
}
```

### è®­ç»ƒå‘½ä»¤

```bash
# åŸºç¡€è®­ç»ƒï¼ˆä»…è½¨è¿¹ç‰¹å¾ï¼‰
python train_bouncenet.py \
    --label_dir labels/ \
    --mode trajectory_only \
    --epochs 100 \
    --batch_size 32 \
    --lr 1e-3 \
    --save_dir ckpts/bouncenet

# èåˆæ¨¡å¼ï¼ˆéœ€è¦è§†é¢‘ï¼‰
python train_bouncenet.py \
    --label_dir labels/ \
    --mode fusion \
    --epochs 100 \
    --batch_size 16 \
    --lr 5e-4
```

### è®­ç»ƒå‚æ•°

| å‚æ•° | é»˜è®¤å€¼ | è¯´æ˜ |
|------|--------|------|
| `--label_dir` | å¿…å¡« | æ ‡æ³¨æ–‡ä»¶ç›®å½• |
| `--mode` | `trajectory_only` | ç‰¹å¾æ¨¡å¼ |
| `--window_size` | 5 | ç‰¹å¾çª—å£ï¼ˆå•ä¾§ï¼‰ |
| `--batch_size` | 32 | æ‰¹æ¬¡å¤§å° |
| `--epochs` | 100 | è®­ç»ƒè½®æ•° |
| `--lr` | 1e-3 | å­¦ä¹ ç‡ |
| `--early_stopping` | 15 | æ—©åœè½®æ•° |
| `--val_split` | 0.2 | éªŒè¯é›†æ¯”ä¾‹ |

### æŸå¤±å‡½æ•°

```python
class BounceNetLoss:
    """
    ç»„åˆæŸå¤±:
    1. Focal Loss: å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
       L_focal = -Î±(1-p)^Î³ * log(p)
    
    2. Confidence Loss: é¼“åŠ±æ­£ç¡®é¢„æµ‹æœ‰é«˜ç½®ä¿¡åº¦
       L_conf = BCE(confidence, is_correct)
    
    Total = L_focal + Î» * L_conf
    """
```

### æ•°æ®å¢å¼º

è®­ç»ƒæ—¶è‡ªåŠ¨åº”ç”¨:
- **æ°´å¹³ç¿»è½¬**: 50% æ¦‚ç‡ç¿»è½¬ x åæ ‡å’Œé€Ÿåº¦
- **å™ªå£°æ³¨å…¥**: åæ ‡æ·»åŠ é«˜æ–¯å™ªå£°
- **æ—¶é—´åç§»**: è½»å¾®çš„æ—¶åºå¹³ç§»

---

## æ¨ç†ä½¿ç”¨

### æ–¹æ³• 1: é€šè¿‡ BounceDetector

```python
from bounce_detection import BounceDetector

# åŠ è½½å¸¦ BounceNet çš„æ£€æµ‹å™¨
detector = BounceDetector(
    bouncenet_ckpt='ckpts/bouncenet/best.pt',
    use_visual_features=False  # ä»…ç”¨è½¨è¿¹ç‰¹å¾
)

# æ£€æµ‹ï¼ˆè‡ªåŠ¨ä½¿ç”¨ BounceNet è¿‡æ»¤ï¼‰
events = detector.detect_from_csv('trajectory.csv')

# æˆ–æ˜¾å¼æ§åˆ¶
events_rule_only = detector.detect(x, y, visibility, use_bouncenet=False)
events_filtered = detector.detect(x, y, visibility, use_bouncenet=True)
```

### æ–¹æ³• 2: ç›´æ¥ä½¿ç”¨ BounceNetPredictor

```python
from bounce_detection import BounceNet, BounceNetPredictor

# åŠ è½½æ¨¡å‹
predictor = BounceNetPredictor.from_checkpoint(
    'ckpts/bouncenet/best.pt',
    device='cuda'
)

# å‡†å¤‡å€™é€‰ï¼ˆæ¥è‡ª Phase 1ï¼‰
candidates = [
    {'frame': 45, 'event_type': 'hit', ...},
    {'frame': 120, 'event_type': 'landing', ...},
]

# åˆ†ç±»
classified = predictor.classify_candidates(
    x, y, visibility, candidates, frames=None
)

# ç»“æœ
for c in classified:
    print(f"Frame {c['frame']}: "
          f"original={c['event_type']}, "
          f"predicted={c['predicted_type']}, "
          f"confidence={c['prediction_confidence']:.2f}, "
          f"is_fp={c['is_false_positive']}")
```

### è¾“å‡ºæ ¼å¼

åˆ†ç±»åçš„å€™é€‰äº‹ä»¶åŒ…å«é¢å¤–å­—æ®µ:

```python
{
    # åŸæœ‰å­—æ®µ
    'frame': 45,
    'event_type': 'hit',      # åŸå§‹è§„åˆ™æ£€æµ‹ç±»å‹
    'rule': 'vy_reversal',
    'confidence': 0.80,
    
    # BounceNet æ·»åŠ çš„å­—æ®µ
    'predicted_type': 'hit',   # BounceNet é¢„æµ‹ç±»å‹
    'prediction_probs': {
        'none': 0.05,
        'landing': 0.15,
        'hit': 0.80
    },
    'prediction_confidence': 0.85,
    'is_false_positive': False  # æ˜¯å¦ä¸ºè¯¯æ£€
}
```

---

## API å‚è€ƒ

### BounceNet

```python
class BounceNet(nn.Module):
    def __init__(self,
                 mode: str = 'trajectory_only',
                 # è½¨è¿¹ç¼–ç å™¨
                 traj_input_dim: int = 5,
                 traj_hidden_dim: int = 64,
                 traj_feature_dim: int = 64,
                 traj_seq_len: int = 11,
                 # è§†è§‰ç¼–ç å™¨
                 patch_size: int = 64,
                 visual_feature_dim: int = 128,
                 use_temporal_patches: bool = True,
                 # åˆ†ç±»å™¨
                 num_classes: int = 3,
                 dropout: float = 0.3):
        ...
    
    def forward(self, traj_features, visual_features=None, 
                return_confidence=False):
        """
        Args:
            traj_features: (N, seq_len, 5) è½¨è¿¹ç‰¹å¾
            visual_features: (N, seq_len, C, H, W) è§†è§‰ç‰¹å¾
            return_confidence: æ˜¯å¦è¿”å›ç½®ä¿¡åº¦
        
        Returns:
            logits: (N, 3) åˆ†ç±» logits
            confidence: (N, 1) ç½®ä¿¡åº¦ï¼ˆå¯é€‰ï¼‰
        """
    
    def predict(self, traj_features, visual_features=None, threshold=0.5):
        """
        Returns:
            {
                'labels': (N,) é¢„æµ‹æ ‡ç­¾,
                'probs': (N, 3) ç±»åˆ«æ¦‚ç‡,
                'confidence': (N,) ç½®ä¿¡åº¦
            }
        """
```

### VisualFeatureExtractor

```python
class VisualFeatureExtractor:
    def __init__(self,
                 patch_size: int = 64,
                 temporal_window: int = 5,
                 use_motion_history: bool = True,
                 img_size: Tuple[int, int] = (512, 288)):
        ...
    
    def extract_features(self, frames, x_coords, y_coords, 
                        center_frame_idx, visibility) -> Dict:
        """æå–å•ä¸ªå€™é€‰çš„ç‰¹å¾"""
    
    def extract_batch_features(self, frames, x_coords, y_coords,
                              candidate_frames, visibility) -> Dict:
        """æ‰¹é‡æå–å¤šä¸ªå€™é€‰çš„ç‰¹å¾"""
    
    def extract_patch(self, frame, x, y, patch_size=None) -> np.ndarray:
        """æå–å•ä¸ª patch"""
    
    def compute_motion_history_image(self, frames, center_frame_idx,
                                    threshold=25) -> np.ndarray:
        """è®¡ç®—è¿åŠ¨å†å²å›¾"""
```

### BounceNetPredictor

```python
class BounceNetPredictor:
    def __init__(self, model, feature_extractor=None,
                 kinematics_calculator=None, device='cuda'):
        ...
    
    @classmethod
    def from_checkpoint(cls, ckpt_path, mode='trajectory_only', device='cuda'):
        """ä»æ£€æŸ¥ç‚¹åŠ è½½"""
    
    def prepare_trajectory_features(self, x, y, visibility, 
                                   candidate_frames, window_size=5,
                                   img_size=(512, 288)) -> torch.Tensor:
        """å‡†å¤‡è½¨è¿¹ç‰¹å¾"""
    
    def classify_candidates(self, x, y, visibility, candidates,
                           frames=None, threshold=0.5) -> List[Dict]:
        """å¯¹å€™é€‰äº‹ä»¶è¿›è¡Œåˆ†ç±»"""
```

---

## å®éªŒç»“æœ

### æ¨¡å‹å¯¹æ¯”

| æ¨¡å¼ | å‚æ•°é‡ | å‡†ç¡®ç‡ | æ¨ç†é€Ÿåº¦ |
|------|--------|--------|---------|
| trajectory_only | 196K | ~92% | 0.5ms/sample |
| visual_only | 350K | ~88% | 15ms/sample |
| fusion | 460K | ~94% | 16ms/sample |

*æ³¨ï¼šç»“æœåŸºäºå†…éƒ¨æµ‹è¯•é›†ï¼Œå®é™…æ€§èƒ½å–å†³äºè®­ç»ƒæ•°æ®è´¨é‡*

### å„ç±»æ€§èƒ½

| ç±»åˆ« | Precision | Recall | F1 |
|------|-----------|--------|-----|
| none | 0.85 | 0.80 | 0.82 |
| landing | 0.90 | 0.92 | 0.91 |
| hit | 0.93 | 0.95 | 0.94 |

### è®­ç»ƒæ›²çº¿ç¤ºä¾‹

```
Epoch 1/100
  Train Loss: 0.8542, Acc: 0.6521
  Val Loss: 0.7123, Acc: 0.7234
  -> New best model saved!

Epoch 50/100
  Train Loss: 0.2134, Acc: 0.9245
  Val Loss: 0.2567, Acc: 0.9123

Epoch 85/100
  Train Loss: 0.1523, Acc: 0.9456
  Val Loss: 0.2234, Acc: 0.9312
  -> New best model saved!

Early stopping at epoch 100
Training complete! Best val acc: 0.9312
```

---

## å¸¸è§é—®é¢˜

### Q: åº”è¯¥é€‰æ‹©å“ªç§æ¨¡å¼ï¼Ÿ

- **trajectory_only**: æ¨èé¦–é€‰ï¼Œé€Ÿåº¦å¿«ï¼Œæ— éœ€è§†é¢‘
- **fusion**: è¿½æ±‚æœ€é«˜ç²¾åº¦æ—¶ä½¿ç”¨
- **visual_only**: è½¨è¿¹æ•°æ®å™ªå£°å¤§æ—¶è€ƒè™‘

### Q: è®­ç»ƒæ•°æ®é‡éœ€è¦å¤šå°‘ï¼Ÿ

- æœ€ä½ï¼š~100 ä¸ªæ ‡æ³¨äº‹ä»¶
- å»ºè®®ï¼š500+ æ ‡æ³¨äº‹ä»¶
- ç†æƒ³ï¼š1000+ æ ‡æ³¨äº‹ä»¶ï¼Œè¦†ç›–å¤šç§æ¯”èµ›åœºæ™¯

### Q: å¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Ÿ

é»˜è®¤ä½¿ç”¨ Focal Lossï¼Œè‡ªåŠ¨å¤„ç†ã€‚ä¹Ÿå¯ä»¥è°ƒæ•´:
```python
criterion = BounceNetLoss(
    class_weights=[0.3, 1.0, 1.0],  # none æƒé‡é™ä½
    use_focal_loss=True,
    focal_gamma=2.0
)
```

### Q: æ¨ç†æ—¶ GPU å†…å­˜ä¸è¶³ï¼Ÿ

```python
# ä½¿ç”¨ CPU
predictor = BounceNetPredictor.from_checkpoint(ckpt_path, device='cpu')

# æˆ–å‡å°æ‰¹æ¬¡
# åœ¨ classify_candidates ä¸­ä¼šè‡ªåŠ¨å¤„ç†
```

---

## å‚è€ƒæ–‡çŒ®

- TrackNetV3: [GitHub](https://github.com/alenzenx/TracknetV3)
- Focal Loss: Lin et al., "Focal Loss for Dense Object Detection", ICCV 2017
- Motion History Image: Davis & Bobick, "The Representation and Recognition of Action Using Temporal Templates", CVPR 1997
